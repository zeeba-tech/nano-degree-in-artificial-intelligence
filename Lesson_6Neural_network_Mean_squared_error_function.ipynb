{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson-6Neural network/Mean squared error function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmZ+HOYq4/+nUIMM2/P5T5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeba-tech/nano-degree-in-artificial-intelligence/blob/main/Lesson_6Neural_network_Mean_squared_error_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKX5i80Mspl"
      },
      "source": [
        "Log-Loss vs Mean Squared Error\n",
        "In the previous section, Luis taught you about the log-loss function. There are many other error functions used for neural networks. Let me teach you another one, called the mean squared error. As the name says, this one is the mean of the squares of the differences between the predictions and the labels. In the following section I'll go over it in detail, then we'll get to implement backpropagation with it on the same student admissions dataset.\n",
        "\n",
        "And as a bonus, we'll be implementing this in a very effective way using matrix multiplication with NumPy!\n",
        "Gradient Descent with Squared Errors\n",
        "We want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the error. A common metric is the sum of the squared errors (SSE):\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esjoBZjJNthS"
      },
      "source": [
        "Gradient Descent with Squared Errors\n",
        "We want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the error. A common metric is the sum of the squared errors (SSE):\n",
        "\n",
        "E = \\frac{1}{2}\\sum_{\\mu} \\sum_j \\left[ y^{\\mu}_j - \\hat{y} ^{\\mu}_j \\right]^2E= \n",
        "2\n",
        "1\n",
        "​\t  \n",
        "μ\n",
        "∑\n",
        "​\t  \n",
        "j\n",
        "∑\n",
        "​\t [y \n",
        "j\n",
        "μ\n",
        "​\t − \n",
        "y\n",
        "^\n",
        "​\t  \n",
        "j\n",
        "μ\n",
        "​\t ] \n",
        "2\n",
        " \n",
        "\n",
        "where \\hat y \n",
        "y\n",
        "^\n",
        "​\t  is the prediction and yy is the true value, and you take the sum over all output units jj and another sum over all data points \\muμ. This might seem like a really complicated equation at first, but it's fairly simple once you understand the symbols and can say what's going on in words.\n",
        "\n",
        "First, the inside sum over jj. This variable jj represents the output units of the network. So this inside sum is saying for each output unit, find the difference between the true value yy and the predicted value from the network \\hat y \n",
        "y\n",
        "^\n",
        "​\t , then square the difference, then sum up all those squares.\n",
        "\n",
        "Then the other sum over \\muμ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
        "\n",
        "The SSE is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.\n",
        "\n",
        "Remember that the output of a neural network, the prediction, depends on the weights\n",
        "\n",
        "\\hat{y}^{\\mu}_j = f \\left( \\sum_i{ w_{ij} x^{\\mu}_i }\\right) \n",
        "y\n",
        "^\n",
        "​\t  \n",
        "j\n",
        "μ\n",
        "​\t =f( \n",
        "i\n",
        "∑\n",
        "​\t w \n",
        "ij\n",
        "​\t x \n",
        "i\n",
        "μ\n",
        "​\t )\n",
        "\n",
        "and accordingly the error depends on the weights\n",
        "\n",
        "E = \\frac{1}{2}\\sum_{\\mu} \\sum_j \\left[ y^{\\mu}_j - f \\left( \\sum_i{ w_{ij} x^{\\mu}_i }\\right) \\right]^2E= \n",
        "2\n",
        "1\n",
        "​\t  \n",
        "μ\n",
        "∑\n",
        "​\t  \n",
        "j\n",
        "∑\n",
        "​\t [y \n",
        "j\n",
        "μ\n",
        "​\t −f( \n",
        "i\n",
        "∑\n",
        "​\t w \n",
        "ij\n",
        "​\t x \n",
        "i\n",
        "μ\n",
        "​\t )] \n",
        "2\n",
        " \n",
        "\n",
        "We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights w_{ij}w \n",
        "ij\n",
        "​\t  that minimize the squared error EE. To do this with a neural network, typically you'd use gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvVhBQFZNfem"
      },
      "source": [
        "Enter Gradient Descent\n",
        "\n",
        "As Luis said, with gradient descent, we take multiple small steps towards our goal. In this case, we want to change the weights in steps that reduce the error. Continuing the analogy, the error is our mountain and we want to get to the bottom. Since the fastest way down a mountain is in the steepest direction, the steps taken should be in the direction that minimizes the error the most. We can find this direction by calculating the gradient of the squared error.\n",
        "\n",
        "Gradient is another term for rate of change or slope. If you need to brush up on this concept, check out Khan Academy's great lectures on the topic.\n",
        "\n",
        "To calculate a rate of change, we turn to calculus, specifically derivatives. A derivative of a function f(x)f(x) gives you another function f'(x)f \n",
        "′\n",
        " (x) that returns the slope of f(x)f(x) at point xx. For example, consider f(x)=x^2f(x)=x \n",
        "2\n",
        " . The derivative of x^2x \n",
        "2\n",
        "  is f'(x) = 2xf \n",
        "′\n",
        " (x)=2x. So, at x = 2x=2, the slope is f'(2) = 4f \n",
        "′\n",
        " (2)=4. Plotting this out, it looks like:\n",
        "\n",
        "\n",
        "Example of a gradient\n",
        "\n",
        "The gradient is just a derivative generalized to functions with more than one variable. We can use calculus to find the gradient at any point in our error function, which depends on the input weights. You'll see how the gradient descent step is derived on the next page.\n",
        "\n",
        "Below I've plotted an example of the error of a neural network with two inputs, and accordingly, two weights. You can read this like a topographical map where points on a contour line have the same error and darker contour lines correspond to larger errors.\n",
        "\n",
        "At each step, you calculate the error and the gradient, then use those to determine how much to change each weight. Repeating this process will eventually find weights that are close to the minimum of the error function, the black dot in the middle.\n",
        "\n",
        "\n",
        "Gradient descent steps to the lowest error\n",
        "\n",
        "Caveats\n",
        "Since the weights will just go wherever the gradient takes them, they can end up where the error is low, but not the lowest. These spots are called local minima. If the weights are initialized with the wrong values, gradient descent could lead the weights into a local minimum, illustrated below.\n",
        "\n",
        "\n",
        "Gradient descent leading into a local minimum\n",
        "\n",
        "There are methods to avoid this, such as using momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3jYt4oxNeZR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}