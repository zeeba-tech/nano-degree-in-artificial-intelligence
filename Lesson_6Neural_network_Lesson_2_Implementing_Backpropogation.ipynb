{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson-6Neural network/Lesson-2 Implementing Backpropogation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpMBRsmznZUqLQwIPYvBoS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeba-tech/nano-degree-in-artificial-intelligence/blob/main/Lesson_6Neural_network_Lesson_2_Implementing_Backpropogation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9INoUQXlSF1W"
      },
      "source": [
        "Implementing backpropagation\n",
        "Now we've seen that the error term for the output layer is\n",
        "\n",
        "\\delta_k = (y_k - \\hat y_k) f'(a_k)δ \n",
        "k\n",
        "​\t =(y \n",
        "k\n",
        "​\t − \n",
        "y\n",
        "^\n",
        "​\t  \n",
        "k\n",
        "​\t )f \n",
        "′\n",
        " (a \n",
        "k\n",
        "​\t )\n",
        "\n",
        "and the error term for the hidden layer is\n",
        "\n",
        "\n",
        "For now we'll only consider a simple network with one hidden layer and one output unit. Here's the general algorithm for updating the weights with backpropagation:\n",
        "\n",
        "Set the weight steps for each layer to zero\n",
        "The input to hidden weights \\Delta w_{ij} = 0Δw \n",
        "ij\n",
        "​\t =0\n",
        "The hidden to output weights \\Delta W_j = 0ΔW \n",
        "j\n",
        "​\t =0\n",
        "For each record in the training data:\n",
        "\n",
        "Make a forward pass through the network, calculating the output \\hat y \n",
        "y\n",
        "^\n",
        "​\t \n",
        "Calculate the error gradient in the output unit, \\delta^o = (y - \\hat y) f'(z)δ \n",
        "o\n",
        " =(y− \n",
        "y\n",
        "^\n",
        "​\t )f \n",
        "′\n",
        " (z) where z = \\sum_j W_j a_jz=∑ \n",
        "j\n",
        "​\t W \n",
        "j\n",
        "​\t a \n",
        "j\n",
        "​\t , the input to the output unit.\n",
        "Propagate the errors to the hidden layer \\delta^h_j = \\delta^o W_j f'(h_j)δ \n",
        "j\n",
        "h\n",
        "​\t =δ \n",
        "o\n",
        " W \n",
        "j\n",
        "​\t f \n",
        "′\n",
        " (h \n",
        "j\n",
        "​\t )\n",
        "Update the weight steps:\n",
        "\n",
        "\\Delta W_j = \\Delta W_j + \\delta^o a_jΔW \n",
        "j\n",
        "​\t =ΔW \n",
        "j\n",
        "​\t +δ \n",
        "o\n",
        " a \n",
        "j\n",
        "​\t \n",
        "\\Delta w_{ij} = \\Delta w_{ij} + \\delta^h_j a_iΔw \n",
        "ij\n",
        "​\t =Δw \n",
        "ij\n",
        "​\t +δ \n",
        "j\n",
        "h\n",
        "​\t a \n",
        "i\n",
        "​\t \n",
        "Update the weights, where \\etaη is the learning rate and mm is the number of records:\n",
        "\n",
        "W_j = W_j + \\eta \\Delta W_j / mW \n",
        "j\n",
        "​\t =W \n",
        "j\n",
        "​\t +ηΔW \n",
        "j\n",
        "​\t /m\n",
        "w_{ij} = w_{ij} + \\eta \\Delta w_{ij} / mw \n",
        "ij\n",
        "​\t =w \n",
        "ij\n",
        "​\t +ηΔw \n",
        "ij\n",
        "​\t /m\n",
        "Repeat for ee epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_szLqjG9SKHB"
      },
      "source": [
        "Backpropagation exercise\n",
        "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
        "\n",
        "Your goals here:\n",
        "\n",
        "Implement the forward pass.\n",
        "Implement the backpropagation algorithm.\n",
        "Update the weights.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "np.random.seed(21)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_hidden = 2  # number of hidden units\n",
        "epochs = 900\n",
        "learnrate = 0.005\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "# Initialize weights\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        ## Forward pass ##\n",
        "        # TODO: Calculate the output\n",
        "        hidden_input =np.dot(x,weights_input_hidden)\n",
        "        hidden_output =sigmoid(hidden_input)\n",
        "        output =sigmoid(np.dot(hidden_output,weights_hidden_output))\n",
        "\n",
        "        ## Backward pass ##\n",
        "        # TODO: Calculate the network's prediction error\n",
        "        error =y-output\n",
        "\n",
        "        # TODO: Calculate error term for the output unit\n",
        "        output_error_term =error*output*(1-output)\n",
        "\n",
        "        ## propagate errors to hidden layer\n",
        "\n",
        "        # TODO: Calculate the hidden layer's contribution to the error\n",
        "        hidden_error =np.dot(output_error_term, weights_hidden_output)\n",
        "        \n",
        "        # TODO: Calculate the error term for the hidden layer\n",
        "        hidden_error_term = hidden_error*hidden_output*(1-hidden_output)\n",
        "        \n",
        "        # TODO: Update the change in weights\n",
        "        del_w_hidden_output +=learnrate*output_error_term*hidden_output\n",
        "        del_w_input_hidden += learnrate*hidden_error_term*x[:,None]\n",
        "    # TODO: Update weights  (don't forget to division by n_records or number of samples)\n",
        "    weights_input_hidden += del_w_input_hidden/n_records\n",
        "    weights_hidden_output +=del_w_hidden_output/n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}