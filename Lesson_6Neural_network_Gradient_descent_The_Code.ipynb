{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson-6Neural network/Gradient descent The Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHIU5WbR66EASFNbCO0tbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeba-tech/nano-degree-in-artificial-intelligence/blob/main/Lesson_6Neural_network_Gradient_descent_The_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPsznuKRYhav"
      },
      "source": [
        "Gradient Descent: The Code\n",
        "From before we saw that one weight update can be calculated as:\n",
        "\n",
        "\\Delta w_i = \\eta \\, \\delta x_iΔw \n",
        "i\n",
        "​\t =ηδx \n",
        "i\n",
        "​\t \n",
        "\n",
        "with the error term \\deltaδ as\n",
        "\n",
        "\\delta = (y - \\hat y) f'(h) = (y - \\hat y) f'(\\sum w_i x_i)δ=(y− \n",
        "y\n",
        "^\n",
        "​\t )f \n",
        "′\n",
        " (h)=(y− \n",
        "y\n",
        "^\n",
        "​\t )f \n",
        "′\n",
        " (∑w \n",
        "i\n",
        "​\t x \n",
        "i\n",
        "​\t )\n",
        "\n",
        "Remember, in the above equation (y - \\hat y)(y− \n",
        "y\n",
        "^\n",
        "​\t ) is the output error, and f'(h)f \n",
        "′\n",
        " (h) refers to the derivative of the activation function, f(h)f(h). We'll call that derivative the output gradient.\n",
        "\n",
        "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function f(h)f(h).\n",
        "\n",
        "# Defining the sigmoid function for activations\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Input data\n",
        "x = np.array([0.1, 0.3])\n",
        "# Target\n",
        "y = 0.2\n",
        "# Input to output weights\n",
        "weights = np.array([-0.8, 0.5])\n",
        "\n",
        "# The learning rate, eta in the weight step equation\n",
        "learnrate = 0.5\n",
        "\n",
        "# the linear combination performed by the node (h in f(h) and f'(h))\n",
        "h = x[0]*weights[0] + x[1]*weights[1]\n",
        "# or h = np.dot(x, weights)\n",
        "\n",
        "# The neural network output (y-hat)\n",
        "nn_output = sigmoid(h)\n",
        "\n",
        "# output error (y - y-hat)\n",
        "error = y - nn_output\n",
        "\n",
        "# output gradient (f'(h))\n",
        "output_grad = sigmoid_prime(h)\n",
        "\n",
        "# error term (lowercase delta)\n",
        "error_term = error * output_grad\n",
        "\n",
        "# Gradient descent step \n",
        "del_w = [ learnrate * error_term * x[0],\n",
        "          learnrate * error_term * x[1]]\n",
        "# or del_w = learnrate * error_term * x\n",
        "Note: If you are wondering where the derivative of the sigmoid function comes from (sigmoid_prime above), check out the derivation in this post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBVwj7YWdxs0"
      },
      "source": [
        "In the quiz below, you'll implement gradient descent in code yourself, although with a few differences (which we'll leave to you to figure out!) from the above example.\n",
        "\n",
        "gradient.py\n",
        "solution.py\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    \"\"\"\n",
        "    # Derivative of the sigmoid function\n",
        "    \"\"\"\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "learnrate = 0.5\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array(0.5)\n",
        "\n",
        "# Initial weights\n",
        "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
        "\n",
        "### Calculate one gradient descent step for each weight\n",
        "### Note: Some steps have been consolidated, so there are\n",
        "###       fewer variable names than in the above sample code\n",
        "\n",
        "# TODO: Calculate the node's linear combination of inputs and weights\n",
        "h = np.dot(x, w)\n",
        "\n",
        "# TODO: Calculate output of neural network\n",
        "nn_output = sigmoid(h)\n",
        "\n",
        "# TODO: Calculate error of neural network\n",
        "error = y - nn_output\n",
        "\n",
        "# TODO: Calculate the error term\n",
        "#       Remember, this requires the output gradient, which we haven't\n",
        "#       specifically added a variable for.\n",
        "error_term = error * sigmoid_prime(h)\n",
        "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
        "#       but you've already calculated it once. You can make this\n",
        "#       code more efficient by calculating the derivative directly\n",
        "#       rather than calling sigmoid_prime, like this:\n",
        "# error_term = error * nn_output * (1 - nn_output)\n",
        "\n",
        "# TODO: Calculate change in weights\n",
        "del_w = learnrate * error_term * x\n",
        "\n",
        "print('Neural Network output:')\n",
        "print(nn_output)\n",
        "print('Amount of Error:')\n",
        "print(error)\n",
        "print('Change in Weights:')\n",
        "print(del_w)\n",
        "  \n"
      ]
    }
  ]
}